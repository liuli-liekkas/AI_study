{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca31aa4",
   "metadata": {},
   "source": [
    "# 线性回归\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1371695",
   "metadata": {},
   "source": [
    "线性模型是实践中经常使用的模型。虽然它主要用于回归，但也用于分类。本文主要讨论回归的线性模型。线性模型根据模型的方便函数对输出进行预测。让我们从高中的一个问题开始这个话题。以下几点之间的关系是什么？\n",
    "(1,3) ，(2,5) ，(3,7) ，(4,9) ，(5,11)\n",
    "这个问题由 x 值(1,2,3,4,5)和 x 值(3,5,7,9,11)对应的输出组成。很容易理解的是，输入和输出之间的简单关系是直线 y = 2x + 1的点，斜率为2。线性模型用于预测相应输入值的未来值。例如，我们可以预测输入(x)的输出(y) = 21 = 10。\n",
    "\n",
    "我们创建一个随机的数据集，并使用 sklearn 库来拟合线性回归。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd60bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa7dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(500, 1)\n",
    "y = 100 + 10 * x + np.random.randn(500, 1)\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x,y)\n",
    "y_pred=linear_regression.predict(x)\n",
    "MSE=np.square(np.subtract(y_pred,y)).mean()\n",
    "\n",
    "print(\"MSE   : \",MSE)\n",
    "print(\"bias  : \", linear_regression.intercept_)\n",
    "print(\"weight: \",linear_regression.coef_)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,y_pred,color='red')\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de668ac",
   "metadata": {},
   "source": [
    "## 均方误差\n",
    "\n",
    "定义损失函数为均方误差(Mean Square Error, MSE)\n",
    "\n",
    "$ MSE = \\dfrac{1}{N} \\displaystyle \\sum_{i=1}^N (y_i- \\hat{y}_i)^2$\n",
    "\n",
    "目标是最小化均方误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f8ef6",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "\n",
    "梯度下降法。其思想是通过迭代更新参数使代价函数最小化。该算法采用随机数初始化权值，并根据学习速率逐渐收敛到最小值。其中最重要的参数之一是学习率。如果选择的学习率太低，达到全局最小(最小成本)需要太多的时间; 如果调整的学习率太高，权重跳跃到最小成本附近，变得不稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_cost(theta,x,y):\n",
    "    m = len(y)\n",
    "    predictions = x.dot(theta)\n",
    "    cost = (1/m) * np.sum(np.square(predictions-y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7198b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gradient_descent(x, y, theta, eta=0.01, iterations=1000):\n",
    "    data_len = len(y)\n",
    "    for it in range(iterations):\n",
    "        xtheta = np.dot(x, theta)\n",
    "        theta = theta - (1 / data_len) * eta * (x.T.dot((xtheta - y)))\n",
    "        cost = cal_cost(theta, x, y)\n",
    "    return print(\"bias  :\", theta[0],\"\\nweight:\", theta[1],\"\\ncost  : \",cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "n_iter = 1000\n",
    "theta = np.random.randn(2, 1)\n",
    "x_b = np.c_[np.ones((len(x), 1)), x]\n",
    "gradient_descent(x_b, y, theta, lr, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2144b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "n_iter = 1000\n",
    "theta = np.random.randn(2, 1)\n",
    "x_b = np.c_[np.ones((len(x), 1)), x]\n",
    "gradient_descent(x_b, y, theta, lr, n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887e3f8c",
   "metadata": {},
   "source": [
    "## 批量梯度下降\n",
    "\n",
    "梯度下降法通过在多特征数据集中使用偏导数最小化成本函数。这意味着成本函数是重新计算每个步骤的权重值，这个过程需要太多的时间。相比之下，在批处理梯度下降法中有一个梯度向量，并且在整个训练集的每个步骤中只更新这个向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def batch_gradient_descent(x,y,theta,eta=0.01, iterations=1000):\n",
    "    data_len = len(x)\n",
    "    for iteration in range(iterations):\n",
    "        xtheta = np.dot(x, theta)\n",
    "        pred=xtheta-y\n",
    "        gradients = 2/data_len * x.T.dot(pred)\n",
    "        theta = theta - eta * gradients\n",
    "        cost = cal_cost(theta, x, y)\n",
    "    return print(\"bias  :\", theta[0],\"\\nweight:\", theta[1],\"\\ncost  : \",cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "n_iter = 1000\n",
    "theta = np.random.randn(2, 1)\n",
    "x_b = np.c_[np.ones((len(x), 1)), x]\n",
    "batch_gradient_descent(x_b,y,theta,lr,n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b3c8d",
   "metadata": {},
   "source": [
    "## 随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ff755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "@timeit\n",
    "def StochasticRegressor(x,y, penalty='eleasticnet',learning_rate='constant',eta0=0.1,max_iter=1000):\n",
    "    sgdr = SGDRegressor(penalty='elasticnet',learning_rate='constant',eta0=0.5,max_iter=1000)\n",
    "    sgdr.fit(x, y)\n",
    "    ypred = sgdr.predict(x)\n",
    "    mse = mean_squared_error(y, ypred)\n",
    "    return  print(\"bias  : \",sgdr.intercept_,\"\\nweight: \",sgdr.coef_, \"\\nMSE   : \", mse)\n",
    "StochasticRegressor(x,y,eta0=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac71e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
